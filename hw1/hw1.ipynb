{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T x_i - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} (X \\theta - y)^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{2}{m} X^T (X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta + \\eta h) - J(\\theta) &\\approx \\eta ||h|| \\nabla J(\\theta) \\\\\n",
    "  &= \\frac{2 \\eta ||h||}{m} X^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\frac{2\\eta}{m} X^T (X\\theta-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Experimentally, we find that step size 0.504 seems about optimal. Anything less, and convergence slows down (the final $\\theta$ has a higher loss); anything higher, and we no longer get convergence (loss tends to $\\infty$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} (X\\theta-y)^T(X\\theta-y) + \\lambda\\theta^T\\theta \\\\\n",
    "\\nabla J(\\theta) &= 2\\left(\\frac{1}{m} X^T(X\\theta-y) + \\lambda\\theta\\right) \\\\\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla J(\\theta) \\\\\n",
    "\\theta &\\leftarrow (1 - 2\\eta\\lambda)\\theta + \\frac{2}{m} X^T(X\\theta-y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In order to introduce the bias term, let us write each feature vector $x_i$ as\n",
    "\n",
    "$$\n",
    "x_i = (B, x_{i,1}, \\dots, x_{i,d})^T = (B, x_{i,-B})^T\n",
    "$$\n",
    "\n",
    "and the parameter vector $\\theta$ as\n",
    "\n",
    "$$\n",
    "\\theta = (\\theta_B, \\theta_1, \\dots, \\theta_d)^T = (\\theta_B, \\theta_{-B})^T.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "h_\\theta(x_i) = \\theta^T x_i = \\theta_B B + \\theta_{-B}^T x_{i,-B},\n",
    "$$\n",
    "\n",
    "so $h_\\theta(x_i)$ has intercept $\\theta_B B$. Square loss forces us to pick an intercept close to its \"true\" value. Therefore, by choosing larger and larger values of $B$, we will end up with smaller and smaller values for $\\theta_B$.\n",
    "\n",
    "This means that in the regularization term $\\lambda\\theta^T\\theta$, $\\theta_B$ will contribute only very little, meaning it gets penalized very little.\n",
    "\n",
    "In other words, by picking $B$ sufficiently large, we can arbitrarily reduce the amount of regularization to which the bias term is subjected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "$\\lambda \\approx 0.023848$ is the optimum. It gives a test loss (non-regularized) of $\\sim 2.44017$.\n",
    "\n",
    "![fig1](fig1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "For deployment, I would select $\\theta$ as determined by learning on the _full_ dataset (train + test), using $\\lambda = 0.023848$ as determined above.\n",
    "\n",
    "This is:\n",
    "\n",
    "```\n",
    "Î¸ = [\n",
    "  9.78838029e-02  2.14963331e-01 -1.11330827e-01 -4.15038772e-01\n",
    "  3.08403974e-01  1.42702036e-01  8.63502429e-01 -1.87940708e-01\n",
    " -4.71476931e-01 -6.55271685e-01  6.96191779e-01  3.90746974e-04\n",
    " -2.94725031e-01 -8.64912546e-02  4.03008811e-01  9.49133825e-01\n",
    "  5.37742074e-01 -3.74080754e-01 -5.39943606e-02 -5.39943606e-02\n",
    " -5.39943606e-02 -4.77378742e-02 -4.77378742e-02 -4.77378742e-02\n",
    " -4.20663239e-02 -4.20663239e-02 -4.20663239e-02 -3.91685909e-02\n",
    " -3.91685909e-02 -3.91685909e-02 -3.74774296e-02 -3.74774296e-02\n",
    " -3.74774296e-02 -2.05474900e-02 -2.05474900e-02 -2.05474900e-02\n",
    " -4.17411489e-02 -4.17411489e-02 -4.17411489e-02 -3.85612202e-02\n",
    " -3.85612202e-02 -3.85612202e-02 -3.68423279e-02 -3.68423279e-02\n",
    " -3.68423279e-02 -3.58124856e-02 -3.58124856e-02 -3.58124856e-02\n",
    "  1.56757947e-01\n",
    "]\n",
    "```\n",
    "\n",
    "It gives a test square loss (non-regularized) of 6.10328."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m f_i(\\theta)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f_i(\\theta) = (h_\\theta(x_i) - y_i)^2 + \\lambda \\theta^T \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E} \\nabla f_i(\\theta)\n",
    "  &= \\sum_{j=1}^m \\mathbb{P}(i=j) \\nabla f_j(\\theta) \\\\\n",
    "  &= \\sum_{j=1}^m \\frac{1}{m} \\nabla f_j(\\theta) \\\\\n",
    "  &= \\nabla \\frac{1}{m} \\sum_{j=1}^m f_j(\\theta) \\\\\n",
    "  &= \\nabla J(\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "First, note that\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla f_i(\\theta)\n",
    "  &= \\nabla \\left( (\\theta^T x_i - y_i)^2 + \\lambda\\theta^T\\theta \\right) \\\\\n",
    "  &= 2 x_i (\\theta^T x_i - y_i) + 2\\lambda\\theta \\\\\n",
    "  &= 2 \\left( (\\theta^T x_i - y_i) x_i + \\lambda \\theta \\right).\n",
    "\\end{align}\n",
    "\n",
    "Now, to update $\\theta$ using SGD, do the following at each step.\n",
    "\n",
    "First pick\n",
    "\n",
    "$$\n",
    "i \\sim \\text{Uniform}(\\{ 1, \\dots, m \\}).\n",
    "$$\n",
    "\n",
    "Then set\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla f_i(\\theta) \\\\\n",
    "  &= \\theta - 2\\eta \\left( (\\theta^T x_i - y_i) x_i + \\lambda \\theta \\right) \\\\\n",
    "  &= (1 - 2 \\eta \\lambda) \\theta - 2 \\eta (\\theta^T x_i - y_i) x_i.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "We again use $\\lambda = 0.023848$. We then find that for fixed step size $\\eta$ in the range from about $0.001$ to $0.004$, SGD outperforms GD (when using 1000 iterations / epochs in either case). Based on a quick check, the optimal value seems to be about $\\eta = 0.002$, giving us a test loss of $2.35$, better than $2.44$ as seen with GD.\n",
    "\n",
    "On the other hand, $\\eta_t \\in \\{ 1/t, 1/\\sqrt{t} \\}$ have not been doing well, despite having $t$ start from 300 to avoid a high initial step size.\n",
    "\n",
    "\n",
    "|$\\eta$|Objective|Test loss|\n",
    "|------|---------|---------|\n",
    "|0.0005|3.731|2.412|\n",
    "|0.001|3.755|2.375|\n",
    "|0.002|3.873|2.350|\n",
    "|0.003|4.031|2.379|\n",
    "|$1/t$|4.243|2.796|\n",
    "|$1/\\sqrt{t}$|1.7e20|1.5e19|\n",
    "\n",
    "![fig2-0](fig2-0.svg)\n",
    "![fig2-1](fig2-1.svg)\n",
    "![fig2-2](fig2-2.svg)\n",
    "![fig2-3](fig2-3.svg)\n",
    "![fig2-4](fig2-4.svg)\n",
    "![fig2-5](fig2-5.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "$$\n",
    "\\eta_t = \\frac{\\eta_0}{1 + \\eta_0 \\lambda t}\n",
    "$$\n",
    "\n",
    "Now with $t$ starting from 1.\n",
    "\n",
    "|$\\eta_0$|Objective|Test loss|\n",
    "|--------|---------|---------|\n",
    "|0.01    |3.7291   |2.4204   |\n",
    "|0.02    |3.7292   |2.4198   |\n",
    "|0.03    |3.7292   |2.4196   |\n",
    "|0.035   |3.7293   |2.4195   |\n",
    "|0.04    |3.7293   |2.4193   |\n",
    "|0.045   |10       |3.8      |\n",
    "|0.05    |1e13     |1e13     |\n",
    "\n",
    "Conclusion: $\\eta_0 \\approx 0.04$ seems to be the sweet spot, though it underperforms (in terms of test loss) fixed step-size $\\eta = 0.02$. But it _outperforms_ any fixed $\\eta$ we have tried _in terms of the objective function_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "We wish to find\n",
    "\n",
    "$$\n",
    "a^* = {\\arg\\min}_a \\mathbb{E} (a-y)^2.\n",
    "$$\n",
    "\n",
    "Now,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E} (a-y)^2\n",
    "  &= \\mathbb{E}(a-\\mathbb{E}y+\\mathbb{E}y-y)^2 \\\\\n",
    "  &= \\mathbb{E}\\left( (a-\\mathbb{E}y)^2 + 2(a-\\mathbb{E}y)(\\mathbb{E}y-y) + (\\mathbb{E}y-y)^2 \\right) \\\\\n",
    "  &= (a-\\mathbb{E}y)^2 + \\mathbb{E}(\\mathbb{E}y-y)^2 \\\\\n",
    "  &= (a-\\mathbb{E}y)^2 + \\text{Var}(y).\n",
    "\\end{align}\n",
    "\n",
    "Clearly, this expression is minimized when $a = a^* = \\mathbb{E}y$.\n",
    "\n",
    "This also shows that its risk (the Bayes risk) is\n",
    "\n",
    "$$\n",
    "R(a^*) = \\mathbb{E} (a^*-y)^2 = \\text{Var}(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2(a)\n",
    "\n",
    "We now wish to find\n",
    "\n",
    "$$\n",
    "f^*(x) = {\\arg\\min}_a \\mathbb{E} \\left[ (a-y)^2 \\mid x \\right].\n",
    "$$\n",
    "\n",
    "This is really the same thing as in exercise 1, except that the distribution of $y$ now depends on $x$. But for a given $f(x)$, $x$ is fixed (so not random), and so the above result applies. Thus:\n",
    "\n",
    "$$\n",
    "f^*(x) = \\mathbb{E}(y \\mid x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2(b)\n",
    "\n",
    "For all $f$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E} (f^*(x)-y)^2\n",
    "  &= \\mathbb{E} \\left[ \\mathbb{E}[(f^*(x)-y)^2 \\mid x] \\right] \\\\\n",
    "  &\\le \\mathbb{E} \\left[ \\mathbb{E}[(f(x)-y)^2 \\mid x] \\right] \\\\\n",
    "  &= \\mathbb{E} (f(x)-y)^2.\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
