{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T x_i - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} (X \\theta - y)^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{2}{m} X^T (X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta + \\eta h) - J(\\theta) &\\approx \\eta ||h|| \\nabla J(\\theta) \\\\\n",
    "  &= \\frac{2 \\eta ||h||}{m} X^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\frac{2\\eta}{m} X^T (X\\theta-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Experimentally, we find that step size 0.504 seems about optimal. Anything less, and convergence slows down (the final $\\theta$ has a higher loss); anything higher, and we no longer get convergence (loss tends to $\\infty$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} (X\\theta-y)^T(X\\theta-y) + \\lambda\\theta^T\\theta \\\\\n",
    "\\nabla J(\\theta) &= 2\\left(\\frac{1}{m} X^T(X\\theta-y) + \\lambda\\theta\\right) \\\\\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla J(\\theta) \\\\\n",
    "\\theta &\\leftarrow (1 - 2\\eta\\lambda)\\theta + \\frac{2}{m} X^T(X\\theta-y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In order to introduce the bias term, let us write each feature vector $x_i$ as\n",
    "\n",
    "$$\n",
    "x_i = (B, x_{i,1}, \\dots, x_{i,d})^T = (B, x_{i,-B})^T\n",
    "$$\n",
    "\n",
    "and the parameter vector $\\theta$ as\n",
    "\n",
    "$$\n",
    "\\theta = (\\theta_B, \\theta_1, \\dots, \\theta_d)^T = (\\theta_B, \\theta_{-B})^T.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "h_\\theta(x_i) = \\theta^T x_i = \\theta_B B + \\theta_{-B}^T x_{i,-B},\n",
    "$$\n",
    "\n",
    "so $h_\\theta(x_i)$ has intercept $\\theta_B B$. Square loss forces us to pick an intercept close to its \"true\" value. Therefore, by choosing larger and larger values of $B$, we will end up with smaller and smaller values for $\\theta_B$.\n",
    "\n",
    "This means that in the regularization term $\\lambda\\theta^T\\theta$, $\\theta_B$ will contribute only very little, meaning it gets penalized very little.\n",
    "\n",
    "In other words, by picking $B$ sufficiently large, we can arbitrarily reduce the amount of regularization to which the bias term is subjected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
