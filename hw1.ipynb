{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_i) - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} \\sum_{i=1}^{m} (\\theta^T x_i - y_i)^2 \\\\\n",
    "  &= \\frac{1}{m} (X \\theta - y)^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{2}{m} X^T (X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta + \\eta h) - J(\\theta) &\\approx \\eta ||h|| \\nabla J(\\theta) \\\\\n",
    "  &= \\frac{2 \\eta ||h||}{m} X^T (X \\theta - y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\frac{2\\eta}{m} X^T (X\\theta-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Experimentally, we find that step size 0.504 seems about optimal. Anything less, and convergence slows down (the final $\\theta$ has a higher loss); anything higher, and we no longer get convergence (loss tends to $\\infty$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta) &= \\frac{1}{m} (X\\theta-y)^T(X\\theta-y) + \\lambda\\theta^T\\theta \\\\\n",
    "\\nabla J(\\theta) &= 2\\left(\\frac{1}{m} X^T(X\\theta-y) + \\lambda\\theta\\right) \\\\\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla J(\\theta) \\\\\n",
    "\\theta &\\leftarrow (1 - 2\\eta\\lambda)\\theta + \\frac{2}{m} X^T(X\\theta-y)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "In order to introduce the bias term, let us write each feature vector $x_i$ as\n",
    "\n",
    "$$\n",
    "x_i = (B, x_{i,1}, \\dots, x_{i,d})^T = (B, x_{i,-B})^T\n",
    "$$\n",
    "\n",
    "and the parameter vector $\\theta$ as\n",
    "\n",
    "$$\n",
    "\\theta = (\\theta_B, \\theta_1, \\dots, \\theta_d)^T = (\\theta_B, \\theta_{-B})^T.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "h_\\theta(x_i) = \\theta^T x_i = \\theta_B B + \\theta_{-B}^T x_{i,-B},\n",
    "$$\n",
    "\n",
    "so $h_\\theta(x_i)$ has intercept $\\theta_B B$. Square loss forces us to pick an intercept close to its \"true\" value. Therefore, by choosing larger and larger values of $B$, we will end up with smaller and smaller values for $\\theta_B$.\n",
    "\n",
    "This means that in the regularization term $\\lambda\\theta^T\\theta$, $\\theta_B$ will contribute only very little, meaning it gets penalized very little.\n",
    "\n",
    "In other words, by picking $B$ sufficiently large, we can arbitrarily reduce the amount of regularization to which the bias term is subjected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "$\\lambda \\approx 0.023848$ is the optimum.\n",
    "\n",
    "![fig1](fig1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "For deployment, I would select $\\theta$ as determined by learning on the _full_ dataset (train + test), using $\\lambda = 0.023848$ as determined above.\n",
    "\n",
    "This is:\n",
    "\n",
    "```\n",
    "Î¸ = [\n",
    "  9.78838029e-02  2.14963331e-01 -1.11330827e-01 -4.15038772e-01\n",
    "  3.08403974e-01  1.42702036e-01  8.63502429e-01 -1.87940708e-01\n",
    " -4.71476931e-01 -6.55271685e-01  6.96191779e-01  3.90746974e-04\n",
    " -2.94725031e-01 -8.64912546e-02  4.03008811e-01  9.49133825e-01\n",
    "  5.37742074e-01 -3.74080754e-01 -5.39943606e-02 -5.39943606e-02\n",
    " -5.39943606e-02 -4.77378742e-02 -4.77378742e-02 -4.77378742e-02\n",
    " -4.20663239e-02 -4.20663239e-02 -4.20663239e-02 -3.91685909e-02\n",
    " -3.91685909e-02 -3.91685909e-02 -3.74774296e-02 -3.74774296e-02\n",
    " -3.74774296e-02 -2.05474900e-02 -2.05474900e-02 -2.05474900e-02\n",
    " -4.17411489e-02 -4.17411489e-02 -4.17411489e-02 -3.85612202e-02\n",
    " -3.85612202e-02 -3.85612202e-02 -3.68423279e-02 -3.68423279e-02\n",
    " -3.68423279e-02 -3.58124856e-02 -3.58124856e-02 -3.58124856e-02\n",
    "  1.56757947e-01\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m f_i(\\theta)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "f_i(\\theta) = (h_\\theta(x_i) - y_i)^2 + \\lambda \\theta^T \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E} \\nabla f_i(\\theta)\n",
    "  &= \\sum_{j=1}^m \\mathbb{P}(i=j) \\nabla f_j(\\theta) \\\\\n",
    "  &= \\sum_{j=1}^m \\frac{1}{m} \\nabla f_j(\\theta) \\\\\n",
    "  &= \\nabla \\frac{1}{m} \\sum_{j=1}^m f_j(\\theta) \\\\\n",
    "  &= \\nabla J(\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "First, note that\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla f_i(\\theta)\n",
    "  &= \\nabla \\left( (\\theta^T x_i - y_i)^2 + \\lambda\\theta^T\\theta \\right) \\\\\n",
    "  &= 2 x_i (\\theta^T x_i - y_i) + 2\\lambda\\theta \\\\\n",
    "  &= 2 \\left( (\\theta^T x_i - y_i) x_i + \\lambda \\theta \\right).\n",
    "\\end{align}\n",
    "\n",
    "Now, to update $\\theta$ using SGD, at each step, first pick\n",
    "\n",
    "$$\n",
    "i \\sim \\text{Uniform}(\\{ 1, \\dots, m \\}).\n",
    "$$\n",
    "\n",
    "Then set\n",
    "\n",
    "\\begin{align}\n",
    "\\theta &\\leftarrow \\theta - \\eta \\nabla f_i(\\theta) \\\\\n",
    "  &= (1 - 2 \\eta \\lambda) \\theta - 2 \\eta (\\theta^T x_i - y_i) x_i.\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
